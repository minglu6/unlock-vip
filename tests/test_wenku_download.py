#!/usr/bin/env python3
"""
CSDNæ–‡åº“æ–‡ç« ä¸‹è½½å’Œè§£æå•å…ƒæµ‹è¯•
ä½¿ç”¨cookieå®Œæˆæ–‡ç« ä¸‹è½½å¹¶æå–å®Œæ•´å†…å®¹
"""
import requests
import json
from bs4 import BeautifulSoup
import re
import urllib3
from typing import Dict, Optional, Tuple
from html import unescape as html_unescape
import markdown
from markdown.extensions import fenced_code, codehilite

# ç¦ç”¨SSLè­¦å‘Š
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)


class WenkuArticleDownloader:
    """CSDNæ–‡åº“æ–‡ç« ä¸‹è½½å™¨"""
    
    def __init__(self, cookies_file: str = "../cookies.json"):
        """åˆå§‹åŒ–ä¸‹è½½å™¨"""
        self.cookies_file = cookies_file
        self.session = None
        self.cookies_dict = None
    
    def load_cookies(self) -> bool:
        """åŠ è½½cookiesæ–‡ä»¶"""
        try:
            with open(self.cookies_file, 'r', encoding='utf-8') as f:
                self.cookies_dict = json.load(f)
            print(f"âœ… æˆåŠŸåŠ è½½cookiesï¼Œå…±{len(self.cookies_dict)}ä¸ª")
            return True
        except FileNotFoundError:
            print(f"âŒ {self.cookies_file}æ–‡ä»¶ä¸å­˜åœ¨")
            return False
        except Exception as e:
            print(f"âŒ åŠ è½½cookieså¤±è´¥: {str(e)}")
            return False
    
    def setup_session(self):
        """è®¾ç½®ä¼šè¯"""
        if not self.cookies_dict:
            if not self.load_cookies():
                return False
        
        self.session = requests.Session()
        
        # è®¾ç½®cookies
        for name, value in self.cookies_dict.items():
            self.session.cookies.set(name, value, domain='.csdn.net')
        
        # è®¾ç½®è¯·æ±‚å¤´
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/141.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Referer': 'https://wenku.csdn.net/',
        }
        self.session.headers.update(headers)
        
        return True
    
    def download_article(self, url: str) -> Tuple[bool, Optional[str]]:
        """
        ä¸‹è½½æ–‡ç« HTML
        
        Returns:
            (success, html_content)
        """
        print(f"\nğŸ“¡ æ­£åœ¨è¯·æ±‚æ–‡åº“é¡µé¢...")
        print(f"URL: {url}")
        
        try:
            response = self.session.get(url, timeout=30, allow_redirects=True, verify=False)
            
            print(f"å“åº”çŠ¶æ€ç : {response.status_code}")
            print(f"æœ€ç»ˆURL: {response.url}")
            print(f"å“åº”å¤§å°: {len(response.content):,} å­—èŠ‚")
            
            if response.status_code != 200:
                print(f"âŒ è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                return False, None
            
            return True, response.text
            
        except Exception as e:
            print(f"âŒ è¯·æ±‚å¼‚å¸¸: {str(e)}")
            return False, None
    
    def extract_article_content(self, html: str) -> Dict:
        """
        ä»HTMLä¸­æå–æ–‡ç« å†…å®¹
        
        Returns:
            {
                'title': str,
                'content': str,
                'text_content': str,
                'publish_time': str,
                'view_count': str,
                'author': str,
                'has_vip_marker': bool,
                'full_html': str,  # å®Œæ•´çš„åŸå§‹HTML
            }
        """
        soup = BeautifulSoup(html, 'html.parser')
        result = {}
        
        # ä¿å­˜å®Œæ•´çš„åŸå§‹HTML
        result['full_html'] = html
        
        # 1. æå–æ ‡é¢˜
        title_tag = soup.find('h1', class_='title')
        if title_tag:
            result['title'] = title_tag.get_text(strip=True)
        else:
            title_tag = soup.find('title')
            result['title'] = title_tag.text.strip() if title_tag else 'æœªçŸ¥æ ‡é¢˜'
        
        print(f"\nğŸ“„ æ–‡ç« æ ‡é¢˜: {result['title']}")
        
        # 2. æå–å…ƒæ•°æ®
        data_items = soup.find_all('span', class_='data-item')
        result['publish_time'] = ''
        result['view_count'] = ''
        result['author'] = ''
        
        for item in data_items:
            text = item.get_text(strip=True)
            if 'æ—¶é—´:' in text:
                result['publish_time'] = text.replace('æ—¶é—´:', '').strip()
            elif 'æµè§ˆ:' in text:
                result['view_count'] = text.replace('æµè§ˆ:', '').strip()
        
        # 3. æ£€æµ‹VIPæ ‡è®°
        vip_text = soup.get_text()
        result['has_vip_marker'] = bool(re.search(r'VIP|ä¼šå‘˜|ä»˜è´¹|è§£é”|é˜…è¯»å…¨æ–‡', vip_text))
        
        # 4. æå–ä¸»è¦å†…å®¹ï¼ˆåªæå–çœŸæ­£çš„æ–‡ç« å†…å®¹ï¼Œä¸å«"é˜…è¯»å…¨æ–‡"æŒ‰é’®ï¼‰
        # æ–‡åº“æ–‡ç« çš„ç»“æ„:
        # <div class="article-box">
        #   <div class="header">æ ‡é¢˜å’Œå…ƒä¿¡æ¯</div>
        #   <div class="cont">
        #     <div class="content-view">
        #       <div class="htmledit_views markdown_views">çœŸæ­£çš„æ–‡ç« å†…å®¹</div>
        #     </div>
        #     <div class="open">é˜…è¯»å…¨æ–‡æŒ‰é’®ï¼ˆéœ€è¦ç§»é™¤ï¼‰</div>
        #   </div>
        # </div>
        
        content_area = None
        
        # ä¼˜å…ˆæŸ¥æ‰¾htmledit_viewsæˆ–markdown_viewsï¼ˆæœ€ç²¾ç¡®çš„å†…å®¹åŒºåŸŸï¼‰
        content_area = soup.find('div', class_=lambda x: x and ('htmledit_views' in x or 'markdown_views' in x))
        
        if content_area:
            print(f"âœ… æ‰¾åˆ°ç²¾ç¡®çš„æ–‡ç« å†…å®¹åŒºåŸŸ: {content_area.get('class')}")
        else:
            # å¤‡ç”¨ï¼šæŸ¥æ‰¾content-viewç„¶åç§»é™¤openæŒ‰é’®
            content_view = soup.find('div', class_='content-view')
            if content_view:
                print(f"âœ… æ‰¾åˆ°content-viewåŒºåŸŸ")
                # æŸ¥æ‰¾å¹¶ç§»é™¤"é˜…è¯»å…¨æ–‡"æŒ‰é’®
                open_btn = content_view.find_next_sibling('div', class_='open')
                if open_btn:
                    print(f"ğŸ—‘ï¸  ç§»é™¤'é˜…è¯»å…¨æ–‡'æŒ‰é’®")
                    open_btn.decompose()
                content_area = content_view
            else:
                print("âš ï¸  æœªæ‰¾åˆ°æ˜ç¡®çš„å†…å®¹åŒºåŸŸï¼Œä½¿ç”¨article-box")
                content_area = soup.find('div', class_='article-box') or soup
        
        # 5. è·å–HTMLå†…å®¹ï¼ˆä¿ç•™åŸå§‹æ ¼å¼ï¼‰
        result['content'] = str(content_area)
        
        # 6. è·å–çº¯æ–‡æœ¬å†…å®¹ï¼ˆç”¨äºç»Ÿè®¡ï¼‰
        # ä¸´æ—¶ç§»é™¤è„šæœ¬å’Œæ ·å¼ä»…ç”¨äºæ–‡æœ¬æå–
        temp_content = BeautifulSoup(str(content_area), 'html.parser')
        for script in temp_content.find_all(['script', 'style']):
            script.decompose()
        
        text_content = temp_content.get_text(separator='\n', strip=True)
        # è§£ç HTMLå®ä½“
        text_content = html_unescape(text_content)
        # æ¸…ç†å¤šä½™ç©ºç™½
        text_content = re.sub(r'\n\s*\n', '\n\n', text_content)
        result['text_content'] = text_content
        
        print(f"ğŸ“Š å†…å®¹ç»Ÿè®¡:")
        print(f"   HTMLé•¿åº¦: {len(result['content']):,} å­—ç¬¦")
        print(f"   æ–‡æœ¬é•¿åº¦: {len(result['text_content']):,} å­—ç¬¦")
        print(f"   å‘å¸ƒæ—¶é—´: {result['publish_time'] or 'æœªçŸ¥'}")
        print(f"   æµè§ˆé‡: {result['view_count'] or 'æœªçŸ¥'}")
        print(f"   VIPæ ‡è®°: {'æ˜¯' if result['has_vip_marker'] else 'å¦'}")
        
        return result
    
    def save_to_file(self, content: Dict, output_file: str) -> bool:
        """ä¿å­˜å†…å®¹åˆ°æ–‡ä»¶ï¼ˆä¿ç•™åŸå§‹æ ¼å¼ï¼Œæ¸²æŸ“Markdownï¼‰"""
        try:
            # æ–¹å¼1: ä¿å­˜å®Œæ•´çš„åŸå§‹HTMLï¼ˆå¤‡ä»½ï¼‰
            original_file = output_file.replace('.html', '_original.html')
            with open(original_file, 'w', encoding='utf-8') as f:
                f.write(content['full_html'])
            print(f"âœ… åŸå§‹HTMLå·²ä¿å­˜åˆ°: {original_file}")
            
            # æ–¹å¼2: ä¿å­˜æ¸²æŸ“åçš„å¯è¯»ç‰ˆæœ¬ï¼ˆMarkdownè½¬HTMLï¼Œä»£ç é«˜äº®ï¼‰
            with open(output_file, 'w', encoding='utf-8') as f:
                soup = BeautifulSoup(content['full_html'], 'html.parser')
                
                # æå–headä¸­çš„CSSæ ·å¼é“¾æ¥
                head_content = soup.find('head')
                css_links = []
                if head_content:
                    for link in head_content.find_all('link', rel='stylesheet'):
                        css_links.append(str(link))
                
                # åªæå–htmledit_views/markdown_viewsåŒºåŸŸï¼ˆçœŸæ­£çš„æ–‡ç« å†…å®¹ï¼‰
                article_content = soup.find('div', class_=lambda x: x and ('htmledit_views' in x or 'markdown_views' in x))
                
                if not article_content:
                    # å¤‡ç”¨æ–¹æ¡ˆï¼šä½¿ç”¨content-viewä½†ç§»é™¤"é˜…è¯»å…¨æ–‡"æŒ‰é’®
                    content_view = soup.find('div', class_='content-view')
                    if content_view:
                        article_content = BeautifulSoup(str(content_view), 'html.parser')
                        for open_btn in article_content.find_all('div', class_='open'):
                            open_btn.decompose()
                
                if not article_content:
                    print("âš ï¸  æœªæ‰¾åˆ°æ–‡ç« å†…å®¹ï¼Œä½¿ç”¨åŸå§‹å†…å®¹")
                    article_content = soup.find('body') or soup
                
                # è·å–Markdownæ–‡æœ¬å†…å®¹
                markdown_text = article_content.get_text()
                
                # ä½¿ç”¨markdownåº“æ¸²æŸ“HTMLï¼ˆæ”¯æŒä»£ç å—ã€è¯­æ³•é«˜äº®ï¼‰
                md = markdown.Markdown(extensions=[
                    'fenced_code',  # æ”¯æŒ```ä»£ç å—
                    'codehilite',   # ä»£ç è¯­æ³•é«˜äº®
                    'tables',       # è¡¨æ ¼æ”¯æŒ
                    'nl2br',        # æ¢è¡Œè½¬<br>
                ])
                rendered_html = md.convert(markdown_text)
                
                print(f"ğŸ¨ Markdownæ¸²æŸ“å®Œæˆï¼ŒHTMLé•¿åº¦: {len(rendered_html)} å­—ç¬¦")
                
                # æ„å»ºå®Œæ•´çš„HTMLæ–‡æ¡£ï¼ˆå¸¦ä»£ç é«˜äº®æ ·å¼ï¼‰
                html_output = f'''<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>{content.get("title", "æ–‡åº“æ–‡ç« ")}</title>
    {chr(10).join(css_links)}
    <style>
        body {{ 
            max-width: 1200px; 
            margin: 0 auto; 
            padding: 20px; 
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Arial, sans-serif;
            background: #fff;
            line-height: 1.8;
            color: #333;
        }}
        .article-meta {{ 
            color: #666; 
            margin: 20px 0; 
            padding: 15px;
            background: #f9f9f9;
            border-radius: 5px;
            border-left: 4px solid #fc5531;
        }}
        .article-meta p {{ margin: 5px 0; }}
        h1 {{ 
            color: #333; 
            margin-bottom: 20px;
            padding-bottom: 10px;
            border-bottom: 2px solid #fc5531;
        }}
        
        /* ä»£ç å—æ ·å¼ï¼ˆGitHubé£æ ¼ï¼‰ */
        pre {{ 
            background: #f6f8fa; 
            padding: 16px; 
            border-radius: 6px; 
            overflow-x: auto;
            border: 1px solid #d0d7de;
            font-size: 14px;
            line-height: 1.45;
            margin: 16px 0;
        }}
        code {{ 
            background: #f6f8fa; 
            padding: 3px 6px; 
            border-radius: 3px; 
            font-family: 'SFMono-Regular', 'Consolas', 'Liberation Mono', 'Menlo', monospace;
            font-size: 85%;
            color: #24292f;
        }}
        pre code {{
            background: transparent;
            padding: 0;
            border-radius: 0;
            font-size: 14px;
            display: block;
        }}
        
        /* Pygmentsä»£ç é«˜äº®æ ·å¼ */
        .codehilite {{ background: #f6f8fa; border-radius: 6px; padding: 16px; margin: 16px 0; }}
        .codehilite pre {{ background: transparent; border: none; padding: 0; margin: 0; }}
        
        /* è¯­æ³•é«˜äº®é¢œè‰² */
        .codehilite .c {{ color: #6a737d; font-style: italic; }} /* Comment */
        .codehilite .k {{ color: #d73a49; font-weight: bold; }} /* Keyword */
        .codehilite .s {{ color: #032f62; }} /* String */
        .codehilite .n {{ color: #24292f; }} /* Name */
        .codehilite .o {{ color: #d73a49; }} /* Operator */
        .codehilite .m {{ color: #005cc5; }} /* Number */
        .codehilite .p {{ color: #24292f; }} /* Punctuation */
        
        /* è¡¨æ ¼æ ·å¼ */
        table {{
            border-collapse: collapse;
            width: 100%;
            margin: 16px 0;
        }}
        table th, table td {{
            border: 1px solid #d0d7de;
            padding: 8px 12px;
            text-align: left;
        }}
        table th {{
            background: #f6f8fa;
            font-weight: 600;
        }}
        
        /* æ®µè½å’Œåˆ—è¡¨ */
        p {{ margin: 12px 0; }}
        ul, ol {{ margin: 12px 0; padding-left: 2em; }}
        li {{ margin: 4px 0; }}
        
        .article-content {{
            font-size: 16px;
            line-height: 1.8;
        }}
    </style>
</head>
<body>
    <h1>{content.get("title", "æ–‡åº“æ–‡ç« ")}</h1>
    <div class="article-meta">
        <p>ğŸ“… å‘å¸ƒæ—¶é—´: {content.get("publish_time", "æœªçŸ¥")}</p>
        <p>ğŸ‘ï¸ æµè§ˆé‡: {content.get("view_count", "æœªçŸ¥")}</p>
    </div>
    <div class="article-content">
        {rendered_html}
    </div>
</body>
</html>'''
                
                f.write(html_output)
            
            print(f"âœ… æ ¼å¼åŒ–HTMLå·²ä¿å­˜åˆ°: {output_file}")
            return True
        except Exception as e:
            print(f"âŒ ä¿å­˜æ–‡ä»¶å¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()
            return False
    
    def download_and_parse(self, url: str, output_file: str = None) -> Optional[Dict]:
        """
        å®Œæ•´æµç¨‹ï¼šä¸‹è½½å¹¶è§£ææ–‡ç« 
        
        Returns:
            æå–çš„æ–‡ç« å†…å®¹å­—å…¸ï¼Œå¤±è´¥è¿”å›None
        """
        print("="*70)
        print("ğŸ“š CSDNæ–‡åº“æ–‡ç« ä¸‹è½½å’Œè§£æ")
        print("="*70)
        
        # 1. è®¾ç½®ä¼šè¯
        if not self.setup_session():
            return None
        
        # 2. ä¸‹è½½æ–‡ç« 
        success, html = self.download_article(url)
        if not success or not html:
            return None
        
        # 3. è§£æå†…å®¹
        content = self.extract_article_content(html)
        
        # 4. ä¿å­˜åˆ°æ–‡ä»¶
        if output_file:
            self.save_to_file(content, output_file)
        
        # 5. æ˜¾ç¤ºå†…å®¹é¢„è§ˆ
        preview_length = 500
        print(f"\nğŸ“– å†…å®¹é¢„è§ˆ (å‰{preview_length}å­—ç¬¦):")
        print("-"*70)
        preview = content['text_content'][:preview_length]
        print(preview)
        if len(content['text_content']) > preview_length:
            print("...")
        print("-"*70)
        
        return content


def test_wenku_download():
    """æµ‹è¯•æ–‡åº“æ–‡ç« ä¸‹è½½"""
    # æµ‹è¯•URL
    test_url = "https://wenku.csdn.net/answer/3pzv32zt84"
    
    # åˆ›å»ºä¸‹è½½å™¨
    downloader = WenkuArticleDownloader()
    
    # ä¸‹è½½å¹¶è§£æ
    result = downloader.download_and_parse(
        url=test_url,
        output_file="wenku_article_complete.html"
    )
    
    # éªŒè¯ç»“æœ
    print("\n" + "="*70)
    if result:
        print("âœ… æµ‹è¯•é€šè¿‡!")
        print(f"\næå–çš„ä¿¡æ¯:")
        print(f"  æ ‡é¢˜: {result['title']}")
        print(f"  å‘å¸ƒæ—¶é—´: {result['publish_time'] or 'æœªçŸ¥'}")
        print(f"  æµè§ˆé‡: {result['view_count'] or 'æœªçŸ¥'}")
        print(f"  å†…å®¹é•¿åº¦: {len(result['text_content']):,} å­—ç¬¦")
        print(f"  HTMLé•¿åº¦: {len(result['content']):,} å­—ç¬¦")
        
        # æ£€æŸ¥å†…å®¹è´¨é‡
        if len(result['text_content']) < 100:
            print("\nâš ï¸  è­¦å‘Š: æå–çš„å†…å®¹å¯èƒ½ä¸å®Œæ•´")
        else:
            print("\nâœ… å†…å®¹æå–å®Œæ•´")
    else:
        print("âŒ æµ‹è¯•å¤±è´¥")
    print("="*70)
    
    return result is not None


if __name__ == "__main__":
    # è¿è¡Œæµ‹è¯•
    success = test_wenku_download()
    
    # è¿”å›æµ‹è¯•ç»“æœ
    exit(0 if success else 1)
